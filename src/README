
txgbe Linux* Base Driver for WangXun(R) Ethernet Network Connection
===============================================================

================================================================================

Mar 7, 2017

================================================================================

Contents
--------

- Important Note
- Overview
- Building and Installation
- Command Line Parameters
- Additional Configurations
- Known Issues/Troubleshooting
- Support
- License


================================================================================


Important Notes
---------------

Disable LRO if enabling ip forwarding or bridging
-------------------------------------------------

WARNING: The txgbe driver supports the Large Receive Offload (LRO) feature.
This option offers the lowest CPU utilization for receives but is completely
incompatible with *routing/ip forwarding* and *bridging*. If enabling ip
forwarding or bridging is a requirement, it is necessary to disable LRO using
compile time options as noted in the LRO section later in this document. The
result of not disabling LRO when combined with ip forwarding or bridging can be
low throughput or even a kernel panic.


Overview
--------

This document describes the txgbe Linux* Base Driver for the 10 Gigabit PCI 
Express Family of Adapters.

The txgbe driver currently supports the CentOS/Redhat 7.x with kernel 3.10, 
CentOS/Redhat 6.x with kernel 2.6, Ubuntu 14.04, Ubuntu 16.10

The txgbe driver currently supports x86_64 only.

This driver is only supported as a loadable module at this time. WangXun is
not supplying patches against the kernel source to allow for static linking of
the drivers.

A version of the driver may already be included by your distribution and/or 
the kernel.org kernel.

For questions related to hardware requirements, refer to the documentation
supplied with your WangXun adapter. All hardware requirements listed apply to
use with Linux.

The following features are now available in supported kernels:
- Native VLANs
- Channel Bonding (teaming)
- SNMP
- Generic Receive Offload
- Data Center Bridging

Adapter teaming is implemented using the native Linux Channel bonding
module. This is included in supported Linux kernels.
Channel Bonding documentation can be found in the Linux kernel source:
/documentation/networking/bonding.txt

Driver information can be obtained using ethtool, lspci, and ifconfig.
Instructions on updating ethtool can be found in the section Additional 
Configurations later in this document.



Identifying Your Adapter
------------------------
The driver in this release is compatible with WangXun Sapphire Dual ports
Ethernet Adapters.


SFP+ Devices with Pluggable Optics
----------------------------------

NOTES:


The following is a list of 3rd party SFP+ modules that have been tested and verified.

Supplier	Type		Part Numbers
--------	----		------------
F-tone      SFP+        FTCS-851X-02D
Avago		SFP+        AFBR-709SMZ


Laser turns off for SFP+ when ifconfig ethX down
------------------------------------------------

"ifconfig ethX down" turns off the laser for SFP+ fiber adapters.
"ifconfig ethX up" turns on the laser.

================================================================================


Building and Installation
-------------------------

The driver can be built from source package.

NOTES:

- For the build to work properly, the currently running kernel MUST match
  the version and configuration of the installed kernel sources. If you have
  just recompiled the kernel reboot the system before building.
- RPM functionality has only been tested in Red Hat distributions.

1. Move the base driver tar file to the directory of your choice. For
   example, use '/home/username/txgbe' or '/usr/local/src/txgbe'.

2. Untar/unzip the archive, where <x.x.x> is the version number for the
   driver tar file:
   tar zxf txgbe-<x.x.x>.tar.gz

3. Change to the driver src directory, where <x.x.x> is the version number
   for the driver tar:
   cd txgbe-<x.x.x>/src/

4. Compile the driver module:
   # make install
   The binary will be installed as:
   /lib/modules/<KERNEL VERSION>/updates/drivers/net/ethernet/wangxun/txgbe/txgbe.ko

   The install location listed above is the default location. This may differ
   for various Linux distributions.

5. Before loading the driver module, make sure that the adapter has been found by pci:
   lspci | grep "8088"
   Note that "8088" is the vendor ID of the adapter.

6. Before loading the driver module, make sure that the driver depended modules ptp and 
   vxlan have been loaded. 
   modprobe ptp
   modprobe vxlan   
   
7. Load the module using the modprobe command:
   modprobe <txgbe> [parameter=port1_value,port2_value]

   Make sure that any older txgbe drivers are removed from the kernel before
   loading the new module:
   rmmod txgbe; modprobe txgbe

8. Assign an IP address to the interface by entering the following,
   where ethX is the interface name that was shown in dmesg after modprobe:
   
   ip address add <IP_address>/<netmask bits> dev ethX

9. Verify that the interface works. Enter the following, where IP_address
   is the IP address for another machine on the same subnet as the interface
   that is being tested:
   ping <IP_address>

NOTE:
   For certain distributions like (but not limited to) RedHat Enterprise
   Linux 7 and Ubuntu, once the driver is installed the initrd/initramfs
   file may need to be updated to prevent the OS loading old versions
   of the txgbe driver. The dracut utility may be used on RedHat
   distributions:
	# dracut --force
   For Ubuntu:
	# update-initramfs -u


================================================================================


Command Line Parameters
-----------------------
If the driver is built as a module, the following optional parameters are used
by entering them on the command line with the modprobe command using this
syntax:
modprobe txgbe [<option>=<VAL1>,<VAL2>,...]

There needs to be a <VAL#> for each network port in the system supported by
this driver. The values will be applied to each instance, in function order.
For example:
modprobe txgbe InterruptThrottleRate=16000,16000

In this case, there are two network ports supported by txgbe in the system.
The default value for each parameter is generally the recommended setting,
unless otherwise noted.

NOTES:
- For more information about the command line parameters, see the application
  note at: http://
- A descriptor describes a data buffer and attributes related to the data
  buffer. This information is accessed by the hardware.


RSS
---
Valid Range: 0-64
0 = Assign up to the lesser value of the number of CPUs and 64
X = Assign X queues, where X is less than or equal to 64. 
RSS also effects the number of transmit queues allocated on 2.6.23 and
newer kernels with CONFIG_NETDEVICES_MULTIQUEUE set in the kernel .config file.
CONFIG_NETDEVICES_MULTIQUEUE only exists from 2.6.23 to 2.6.26. Other options
enable multiqueue in 2.6.27 and newer kernels.


Multiqueue
----------
Valid Range:
0, 1
0 = Disables Multiple Queue support
1 = Enabled Multiple Queue support (a prerequisite for RSS)

IntMode
-------
Valid Range: 0-2 (0 = Legacy Int, 1 = MSI and 2 = MSI-X)
IntMode controls allow load time control over the type of interrupt
registered for by the driver. MSI-X is required for multiple queue
support, and some kernels and combinations of kernel .config options
will force a lower level of interrupt support.
'cat /proc/interrupts' will show different values for each type of interrupt.


InterruptThrottleRate
---------------------
Valid Range:
0=off
1=dynamic
<min_ITR>-<max_ITR>
Interrupt Throttle Rate controls the number of interrupts each interrupt
vector can generate per second. Increasing ITR lowers latency at the cost of
increased CPU utilization, though it may help throughput in some circumstances.
0 = Setting InterruptThrottleRate to 0 turns off any interrupt moderation
  and may improve small packet latency. However, this is generally not
  suitable for bulk throughput traffic due to the increased CPU utilization
  of the higher interrupt rate.
  NOTES:
  - disabling InterruptThrottleRate
    will also result in the driver disabling HW RSC.
  - disabling InterruptThrottleRate will also
    result in disabling LRO (Large Receive Offloads).
1 = Setting InterruptThrottleRate to Dynamic mode attempts to moderate
  interrupts per vector while maintaining very low latency. This can
  sometimes cause extra CPU utilization. If planning on deploying txgbe
  in a latency sensitive environment, this parameter should be considered.
<min_ITR>-<max_ITR> = 980-500000
  Setting InterruptThrottleRate to a value greater or equal to <min_ITR>
  will program the adapter to send at most that many interrupts
  per second, even if more packets have come in. This reduces interrupt load
  on the system and can lower CPU utilization under heavy load, but will
  increase latency as packets are not processed as quickly.



LLI (Low Latency Interrupts)
----------------------------

LLI allows for immediate generation of an interrupt upon processing receive
packets that match certain criteria as set by the parameters described below.
LLI parameters are not enabled when Legacy interrupts are used. You must be
using MSI or MSI-X (see cat /proc/interrupts) to successfully use LLI.


LLIPort
-------
Valid Range: 0-65535
LLI is configured with the LLIPort command-line parameter, which specifies
which TCP port should generate Low Latency Interrupts.
For example, using LLIPort=80 would cause the board to generate an immediate
interrupt upon receipt of any packet sent to TCP port 80 on the local machine.
WARNING: Enabling LLI can result in an excessive number of interrupts/second
that may cause problems with the system and in some cases may cause a kernel
panic.


LLISize
-------
Valid Range: 0-1500
LLISize causes an immediate interrupt if the board receives a packet smaller
than the specified size.


LLIEType
--------
Valid Range: 0-0x8FFF
This parameter specifies the Low Latency Interrupt (LLI) Ethernet protocol type.


LLIVLANP
--------

Valid Range: 0-7

This parameter specifies the LLI on VLAN priority threshold.


Flow Control
------------

Ethernet Flow Control (IEEE 802.3x) can be configured with ethtool to enable
receiving and transmitting pause frames for txgbe. When transmit is enabled,
pause frames are generated when the receive packet buffer crosses a predefined
threshold. When receive is enabled, the transmit unit will halt for the time
delay specified when a pause frame is received. 

Flow Control is enabled by default.

Use ethtool to change the flow control settings.

ethtool:
ethtool -A eth? autoneg off rx off tx off


NOTE: You must have a flow control capable link partner.


Ethernet Flow Director
-------------------------------
NOTE: Flow director parameters are only supported on kernel versions 2.6.30 or
newer.

The Flow Director performs the following tasks:

  - Directs receive packets according to their flows to different queues.
  - Enables tight control on routing a flow in the platform.
  - Matches flows and CPU cores for flow affinity.
  - Supports multiple parameters for flexible flow classification and load
    balancing.

The adapter support two types of filtering modes
  - Perfect filters
    The hardware checks a match between the masked fields of the received 
	packets and the programmed filters.	
  - Signature filters
    The hardware checks a match between a hash-based signature of
    the masked fields of the received packet.
	
	
NOTES:

  - The Flow Director is enabled only if the kernel supports multiple
    transmit queues.
  - An included script (set_irq_affinity) automates setting the IRQ to
    CPU affinity.
  - Flow director masking works in the opposite manner from subnet masking. In
    the following command:
	#ethtool -N eth11 flow-type ip4 src-ip 172.4.1.2 m 255.0.0.0 dst-ip \
	172.21.1.1 m 255.128.0.0 action 31
    The src-ip value that is written to the filter will be 0.4.1.2, not
    172.0.0.0 as might be expected. Similarly, the dst-ip value written to the
    filter will be 0.21.1.1, not 172.0.0.0.

ethtool commands:

  - To change the mode of the Flow Director

	# ethtool -K ethX ntuple <on|off>

	When disabling ntuple filters, the signature filter mode is enabled. 
	When enabling ntuple filters, the perfect filter mode is enabled.
	Filters must be re-added if they are
	needed when ntuple is re-enabled.

  - To add a filter that directs packet to queue 2, use -U or -N switch

	# ethtool -N ethX flow-type tcp4 src-ip 192.168.10.1 dst-ip \
	192.168.10.2 src-port 2000 dst-port 2001 action 2 [loc 1]

  - To see the list of filters currently present
	# ethtool <-u|-n> ethX


Perfect Filter
--------------

Perfect filter is an interface to load the filter table that funnels all flow
into queue_0 unless an alternative queue is specified using "action". In that
case, any flow that matches the filter criteria will be directed to the
appropriate queue.

Support for Virtual Function (VF) is through the user data field. ethtool must
be updated to the version built for the 2.6.40 kernel. Perfect Filter is
supported on all kernels 2.6.30 and later. Rules may be deleted from the table
itself. This is done using "ethtool -U ethX delete N", where N is the rule
number to be deleted.

NOTE: Flow Director Perfect Filters can run in single queue mode when SR-IOV
is enabled or when DCB is enabled.

If the queue is defined as -1, the filter will drop matching packets.

To account for filter matches and misses, there are two stats in ethtool:
fdir_match and fdir_miss. In addition, rx_queue_N_packets shows the number of
packets processed by the Nth queue.

NOTES:
- Receive Packet Steering (RPS) and Receive Flow Steering (RFS) are not
  compatible with Flow Director. If Flow Director is enabled, these will be
  disabled
- For VLAN Masks only four masks are supported.
- Once a rule is defined, you must supply the same fields and masks (if
  masks are specified).


Support for UDP RSS
-------------------

This feature adds an ON/OFF switch for hashing over certain flow types. Only
UDP can be turned on. The default setting is disabled.

Only support for enabling/disabling hashing on ports for UDP over IPv4 (UDP4) or
IPv6 (UDP6) is supported.

NOTE: Fragmented packets may arrive out of order when RSS UDP support is
configured.

Supported Ethtool Commands and Options:
  -n --show-nfc
    Retrieves the receive network flow classification configurations.
  rx-flow-hash tcp4|udp4|ah4|esp4|sctp4|tcp6|udp6|ah6|esp6|sctp6
    Retrieves the hash options for the specified network traffic type.
  -N --config-nfc
    Configures the receive network flow classification.
  rx-flow-hash tcp4|udp4|ah4|esp4|sctp4|tcp6|udp6|ah6|esp6|sctp6
  m|v|t|s|d|f|n|r...
    Configures the hash options for the specified network traffic type.
	udp4	UDP over IPv4
	udp6	UDP over IPv6
	f	Hash on bytes 0 and 1 of the Layer 4 header of the rx packet.
	n	Hash on bytes 2 and 3 of the Layer 4 header of the rx packet.

The following is an example using udp4 (UDP over IPv4):
  - To include UDP port numbers in RSS hashing run:
    ethtool -N ethX rx-flow-hash udp4 sdfn

  - To exclude UDP port numbers from RSS hashing run:
    ethtool -N ethX rx-flow-hash udp4 sd

  - To display UDP hashing current configuration run:
    ethtool -n ethX rx-flow-hash udp4

The results of running that call will be the following, if UDP hashing is
enabled.

  UDP over IPV4 flows use these fields for computing Hash flow key:
    IP SA
    IP DA
    L4 bytes 0 & 1 [TCP/UDP src port]
    L4 bytes 2 & 3 [TCP/UDP dst port]

The results if UDP hashing is disabled are shown below.
  UDP over IPV4 flows use these fields for computing Hash flow key:
    IP SA
    IP DA

Parameters FdirPballoc and AtrSampleRate impact Flow Director.


FdirPballoc
-----------
Valid Range: 1-3
Specifies the Flow Director allocated packet buffer size.
1 = 64k
2 = 128k
3 = 256k


AtrSampleRate
-------------
Valid Range: 0-255
This parameter is used with the signature mode Flow Director and is the 
software ATR transmit packet sample rate. 
For example, when AtrSampleRate is set to 20, every 20th
packet looks to see if the packet will create a new flow. A value of 0
indicates that ATR should be disabled and no samples will be taken.


max_vfs
-------
Valid Range: 1-63
If the value is greater than 0 it will also force the VMDq parameter to be 1
or more.

NOTE: This parameter is only used on kernel 3.7.x and below. On kernel 3.8.x
and above, use sysfs to enable VFs. For example:
#echo $num_vf_enabled > /sys/class/net/$dev/device/sriov_numvfs	//enable VFs
#echo 0 > /sys/class/net/$dev/device/sriov_numvfs	//disable VFs

The parameters for the driver are referenced by position. Thus, if you have a
dual port adapter, or more than one adapter in your system, and want N virtual
functions per port, you must specify a number for each port with each parameter
separated by a comma. For example:
  modprobe txgbe max_vfs=4,1
NOTE: Caution must be used in loading the driver with these parameters.
Depending on your system configuration, number of slots, etc., it is impossible
to predict in all cases where the positions would be on the command line.
This parameter adds support for SR-IOV. It causes the driver to spawn up to
max_vfs worth of virtual functions.
NOTE: When either SR-IOV mode or VMDq mode is enabled, hardware VLAN
filtering and VLAN tag stripping/insertion will remain enabled. Please remove
the old VLAN filter before the new VLAN filter is added. For example,
ip link set eth0 vf 0 vlan 100	// set vlan 100 for VF 0
ip link set eth0 vf 0 vlan 0	// Delete vlan 100
ip link set eth0 vf 0 vlan 200	// set a new vlan 200 for VF 0

With kernel 3.6, the driver supports the simultaneous usage of max_vfs and DCB
features, subject to the constraints described below. Prior to kernel 3.6, the
driver did not support the simultaneous operation of max_vfs greater than 0
and the DCB features (multiple traffic classes utilizing Priority Flow Control
and Extended Transmission Selection).

When DCB is enabled, network traffic is transmitted and received through
multiple traffic classes (packet buffers in the NIC). The traffic is
associated with a specific class based on priority, which has a value of 0
through 7 used in the VLAN tag. When SR-IOV is not enabled, each traffic class
is associated with a set of receive/transmit descriptor queue pairs. The
number of queue pairs for a given traffic class depends on the hardware
configuration. When SR-IOV is enabled, the descriptor queue pairs are grouped
into pools. The Physical Function (PF) and each Virtual Function (VF) is
allocated a pool of receive/transmit descriptor queue pairs. When multiple
traffic classes are configured (for example, DCB is enabled), each pool
contains a queue pair from each traffic class. When a single traffic class is
configured in the hardware, the pools contain multiple queue pairs from the
single traffic class.

The number of VFs that can be allocated depends on the number of traffic
classes that can be enabled. The configurable number of traffic classes for
each enabled VF is as follows:
0 - 15 VFs = Up to 8 traffic classes, depending on device support
16 - 31 VFs = Up to 4 traffic classes
32 - 63 VFs = 1 traffic class

When VFs are configured, the PF is allocated one pool as well. The PF supports
the DCB features with the constraint that each traffic class will only use a
single queue pair. When zero VFs are configured, the PF can support multiple
queue pairs per traffic class.


Configuring SR-IOV for improved network security
------------------------------------------------

In a virtualized environment, on WangXun(R) Server Adapters that support SR-IOV,
the virtual function (VF) may be subject to malicious behavior. Software-
generated layer two frames, like IEEE 802.3x (link flow control), IEEE 802.1Qbb
(priority based flow-control), and others of this type, are not expected and
can throttle traffic between the host and the virtual switch, reducing
performance. To resolve this issue, configure all SR-IOV enabled ports for
VLAN tagging. This configuration allows unexpected, and potentially malicious,
frames to be dropped.


Configuring VLAN tagging on SR-IOV enabled adapter ports
--------------------------------------------------------

To configure VLAN tagging for the ports on an SR-IOV enabled adapter,
use the following command. The VLAN configuration should be done 
before the VF driver is loaded or the VM is booted.

$ ip link set dev <PF netdev id> vf <id> vlan <vlan id>

For example, the following instructions will configure PF eth0 and 
the first VF on VLAN 10.
$ ip link set dev eth0 vf 0 vlan 10
.


LRO
---
Valid Range: 0(off), 1(on)
Large Receive Offload (LRO) is a technique for increasing inbound throughput
of high-bandwidth network connections by reducing CPU overhead. It works by
aggregating multiple incoming packets from a single stream into a larger
buffer before they are passed higher up the networking stack, thus reducing
the number of packets that have to be processed. LRO combines multiple
Ethernet frames into a single receive in the stack, thereby potentially
decreasing CPU utilization for receives.
TXGBE_NO_LRO is a compile time flag. The user can enable it at compile time to add
support for LRO from the driver. The flag is used by adding
CFLAGS_EXTRA="-DTXGBE_NO_LRO" to the make file when it's being compiled.
# make CFLAGS_EXTRA="-DTXGBE_NO_LRO" install
You can verify that the driver is using LRO by looking at these counters in
ethtool:
- lro_aggregated - counts total packets that were combined
- lro_flushed - counts the number of packets flushed out of LRO
NOTE: IPv6 and UDP are not supported by LRO.


DMAC
----
Valid Range: 0, 41-10000
This parameter enables or disables DMA Coalescing feature. Values are in
microseconds and set the internal DMA Coalescing internal timer.
DMAC is available on WangXun(R) based adapters.
DMA (Direct Memory Access) allows the network device to move packet data
directly to the system's memory, reducing CPU utilization. However, the
frequency and random intervals at which packets arrive do not allow the
system to enter a lower power state. DMA Coalescing allows the adapter
to collect packets before it initiates a DMA event. This may increase
network latency but also increases the chances that the system will enter
a lower power state.
Turning on DMA Coalescing may save energy with kernel 2.6.32 and newer.
DMA Coalescing must be enabled across all active ports in order to save
platform power.
InterruptThrottleRate (ITR) should be set to dynamic. When ITR=0, DMA
Coalescing is automatically disabled.
A whitepaper containing information on how to best configure your platform is
available on the WangXun website.

vxlan_rx
----
Valid Range:0(Disable), 1(Enable)
This parameter enables or disables support for VXLAN rx checksum offload

CloudSwitch
----
Valid Range: 0-1 0 = disable Cloud Switch, 1 = enable Cloud Switch
This parameter enables or disables Cloud Switch.
================================================================================


Additional Features and Configurations
-------------------------------------------


Configuring the Driver on Different Distributions
-------------------------------------------------

Configuring a network driver to load properly when the system is started is
distribution dependent. Typically, the configuration process involves adding
an alias line to /etc/modules.conf or /etc/modprobe.conf as well as editing
other system startup scripts and/or configuration files. Many popular Linux
distributions ship with tools to make these changes for you. To learn the
proper way to configure a network device for your system, refer to your
distribution documentation. If during this process you are asked for the
driver or module name, the name for the Base Driver is txgbe.

For example, if you install the txgbe driver for two adapters (eth0
and eth1) and want to set the interrupt mode to MSI-X and MSI, respectively,
add the following to modules.conf or /etc/modprobe.conf:
alias eth0 txgbe
alias eth1 txgbe
options txgbe InterruptThrottleRate=3,1


Viewing Link Messages
---------------------

Link messages will not be displayed to the console if the distribution is
restricting system messages. In order to see network driver link messages on
your console, set dmesg to eight by entering the following:
dmesg -n 8

NOTE: This setting is not saved across reboots.


Jumbo Frames
------------
Jumbo Frames support is enabled by changing the Maximum Transmission Unit
(MTU) to a value larger than the default value of 1500.

Use the ifconfig command to increase the MTU size. For example, enter the
following where <x> is the interface number:

   ifconfig eth<x> mtu 9000 up

NOTES:
- The maximum MTU setting for Jumbo Frames is 9710. This value coincides
  with the maximum Jumbo Frames size of 9728 bytes.
- This driver will attempt to use multiple page sized buffers to receive
  each jumbo packet. This should help to avoid buffer starvation issues
  when allocating receive packets.
- For network connections, if you are enabling jumbo frames in a
  virtual function (VF), jumbo frames must first be enabled in the physical
  function (PF). The VF MTU setting cannot be larger than the PF MTU.


ethtool
-------
The driver utilizes the ethtool interface for driver configuration and
diagnostics, as well as displaying statistical information. The latest
ethtool version is required for this functionality. Download it at
http://ftp.kernel.org/pub/software/network/ethtool/


Speed and Duplex Configuration
------------------------------

In addressing speed and duplex configuration issues, you need to
distinguish between copper-based adapters and fiber-based adapters.

In the default mode, an WangXun(R) Network Adapter using copper connections
will attempt to auto-negotiate with its link partner to determine the best
setting. If the adapter cannot establish link with the link partner using
auto-negotiation, you may need to manually configure the adapter and link
partner to identical settings to establish link and pass packets. This
should only be needed when attempting to link with an older switch that
does not support auto-negotiation or one that has been forced to a specific
speed or duplex mode. Your link partner must match the setting you choose.

Speed and Duplex are configured through the ethtool* utility. ethtool is
included with all versions of Red Hat after Red Hat 7.2. For other Linux
distributions, download and install ethtool from the following website:

   http://ftp.kernel.org/pub/software/network/ethtool/

Caution: Only experienced network administrators should force speed and
duplex manually. The settings at the switch must always match the adapter
settings. Adapter performance may suffer or your adapter may not
operate if you configure the adapter differently from your switch.

An Sapphire Network Adapter using fiber-based connections, however, will not
attempt to auto-negotiate with its link partner since those adapters operate
only in full duplex and only at their native speed.


Hardware Receive Side Coalescing (HW RSC)
-----------------------------------------

Sapphire adapters support HW RSC, which can merge multiple
frames from the same IPv4 TCP/IP flow into a single structure that can span
one or more descriptors. It works similarly to Software Large Receive Offload
technique. By default HW RSC is enabled and SW LRO cannot be used for Sapphire
adapters unless HW RSC is disabled.

TXGBE_NO_HW_RSC is a compile time flag. The user can enable it at compile time
to remove support for HW RSC from the driver. The flag is used by adding
CFLAGS_EXTRA="-DTXGBE_NO_HW_RSC" to the make file when it is being compiled.
make CFLAGS_EXTRA="-DTXGBE_NO_HW_RSC" install

You can verify that the driver is using HW RSC by looking at the counter in
ethtool:

- hw_rsc_count. This counts the total number of Ethernet packets that were
  being combined.


MAC and VLAN anti-spoofing feature
----------------------------------

When a malicious driver attempts to send a spoofed packet, it is dropped by
the hardware and not transmitted.

An interrupt is sent to the PF driver notifying it of the spoof attempt.
When a spoofed packet is detected, the PF driver will send the following
message to the system log (displayed by the "dmesg" command):
txgbe ethX: txgbe_spoof_check: n spoofed packets detected
where "x" is the PF interface number; and "n" is number of spoofed packets.
NOTE: This feature can be disabled for a specific Virtual Function (VF).
ip link set <pf dev> vf <vf id> spoofchk {off|on}


IPRoute2 Tool for setting MAC address, VLAN and rate limit
----------------------------------------------------------

You can set a MAC address of a Virtual Function (VF), a default VLAN and the
rate limit using the IProute2 tool. Download the latest version of the
iproute2 tool from Sourceforge if your version does not have all the features
you require.


Wake on LAN Support (WoL)
-------------------------

Some adapters do not support Wake on LAN. To determine if your adapter
supports Wake on LAN, run
  ethtool ethX




IEEE 1588 Precision Time Protocol (PTP) Hardware Clock (PHC)
------------------------------------------------------------

Precision Time Protocol (PTP) is used to synchronize clocks in a computer
network and is supported in the txgbe driver.


VXLAN Overlay HW Offloading
---------------------------

Virtual Extensible LAN (VXLAN) allows you to extend an L2 network over an L3
network, which may be useful in a virtualized or cloud environment. Some WangXun(R)
Ethernet Network devices perform VXLAN processing, offloading it from the
operating system. This reduces CPU utilization.
 
VXLAN offloading is controlled by the tx and rx checksum offload options
provided by ethtool. That is, if tx checksum offload is enabled, and the adapter
has the capability, VXLAN offloading is also enabled. If rx checksum offload is
enabled, then the VXLAN packets rx checksum will be offloaded, unless the module
parameter vxlan_rx=0,0 was used to specifically disable the VXLAN rx offload.
 
VXLAN Overlay HW Offloading is enabled by default. To view and configure VXLAN
on a VXLAN-overlay offload enabled device, use the following
command:

  # ethtool -k ethX
   (This command displays the offloads and their current state.)


Virtual Function (VF) TX Rate Limit
-----------------------------------

Virtual Function (VF) TX rate limit is configured with an ip command from the
PF interface.
  # ip link set eth0 vf 0 rate 1000
    (This command sets TX Rate Limit of 1000Mbps for VF 0)

Note that the limit is set per queue and not for the entire VF interface.


================================================================================


Known Issues/Troubleshooting
----------------------------

NOTE: After installing the driver, if your Ethernet Network Connection
is not working, verify that you have installed the correct driver.



LRO and iSCSI Incompatibility
-----------------------------

LRO is incompatible with iSCSI target or initiator traffic. A panic may occur
when iSCSI traffic is received through the txgbe driver with LRO enabled. To
workaround this, the driver should be built and installed with:
# make CFLAGS_EXTRA=-DTXGBE_NO_LRO install


UDP Stress Test Dropped Packet Issue
------------------------------------

Under small packet UDP stress with the txgbe driver, the system may
drop UDP packets due to socket buffers being full. Setting the driver Flow
Control variables to the minimum may resolve the issue. You may also try
increasing the kernel's default buffer sizes by changing the values in

  /proc/sys/net/core/rmem_default and rmem_max


DCB: Generic segmentation offload on causes bandwidth allocation issues
-----------------------------------------------------------------------

In order for DCB to work correctly, Generic Segmentation Offload (GSO), also
known as software TSO, must be disabled using ethtool. Since the hardware
supports TSO (hardware offload of segmentation), GSO will not be running by
default. The GSO state can be queried with ethtool using ethtool -k ethX.

Txgbe driver supports 64 queues on a platform with more than 64 cores.

Due to known hardware limitations, RSS can only filter in a maximum of 64
receive queues.


Disable GRO when routing/bridging
---------------------------------

Due to a known kernel issue, GRO must be turned off when routing/bridging. GRO
can be turned off via ethtool.
ethtool -K ethX gro off

where ethX is the ethernet interface being modified.


Lower than expected performance
-------------------------------

Some PCIe x8 slots are actually configured as x4 slots. These slots have
insufficient bandwidth for full line rate with dual port and quad port
devices. In addition, if you put a PCIe Generation 3-capable adapter
into a PCIe Generation 2 slot, you cannot get full bandwidth. The driver
detects this situation and writes the following message in the system log:

"PCI-Express bandwidth available for this card is not sufficient for optimal
performance. For optimal performance a x8 PCI-Express slot is required."

If this error occurs, moving your adapter to a true PCIe Generation 3 x8 slot
 will resolve the issue.


================================================================================


Support
-------
If you got any problem, contact Wangxun support team via support@trustnetic.com

================================================================================


License
-------
  This program is free software; you can redistribute it and/or modify it
  under the terms and conditions of the GNU General Public License,
  version 2, as published by the Free Software Foundation.

  This program is distributed in the hope it will be useful, but WITHOUT
  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
  more details.

  The full GNU General Public License is included in this distribution in
  the file called "COPYING".

WangXun(R) 10 Gigabit Ethernet Network Driver
WangXun(R) 10 Gigabit Virtual Function Network Driver
Copyright(c) 2015 - 2017 Beijing WangXun Technology Co., Ltd.
================================================================================
